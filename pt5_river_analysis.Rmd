---
title: "River Station Analysis"
author: "Andrew Cameron"
date: "2024-11-11"
output: html_document
---

"The analysis of trends in the DEQ reservoir data got me wondering how these compare to trends observed downstream and at a larger spatial scale (riversheds).
I was able to find data for 18 river stations within VA.  Many of the reservoirs are located within the watersheds of these rivers.
The river data have been heavily scrutinized by the USGS and others looking at trends in nutrient loads to CB.  But in order to have comparable numbers, we need to analyze these in the same way we analyzed the reservoir data.
Specifically, we need to reduce the data to May-Oct average values for each year and then run a mblm across years (>1999) for each station.  The variables we are interested in are TSS (USGS #530), TN (600), TP (665) and CHLa (70951, though there may not be much of this data).
The products I would like to get from this are:
1. long-term min,mean,median,max,Nobs for each of the variables by station.
2. trends results (slopes, SE and p values).
3. plots depicting data with trend lines." - Paul (11/11/2024)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
source("C:/Users/andre/my_functions.R")

```

# Load and Filter Data

```{r}
# Load and filter raw data to include only the relevant parameter codes.
data <- openxlsx::read.xlsx("data/river_data/River Stations.xlsx", sheet = 2) %>%
  mutate(date = convertExcelDateTime(., "sample_dt")) %>%
  filter(parm_cd %in% c(530, 600, 665, 70951)) %>%
  filter(date >= "1999-01-01") %>%
  filter(date <= "2023-12-31") %>%
  mutate(variable = case_when(
    parm_cd == 530 ~ "TSS",
    parm_cd == 600 ~ "TN",
    parm_cd == 665 ~ "TP",
    parm_cd == 70951 ~ "CHLa"
  ))

# Check for any duplicates
index <- which(duplicated(data %>% select(site_no, date, variable)))
length(index)  # 1 duplicate

data <- data %>%
  select(site_no, date, variable, result_va) %>%
  group_by(site_no, variable, date) %>%
  summarize(result_va = mean(result_va, na.rm = TRUE), .groups = "drop") %>%  # treat duplicate
  pivot_wider(names_from = variable, values_from = result_va) %>%
  mutate(Month = month(date),
         Year = year(date)) %>%
  filter(Month >= 5 & Month <= 10)

```

# Select for Station-Years w/ Nobs >= 3

```{r}
# --------- Remove site years that do not have a minimum of 3 measurements in May-Oct ------------
  ## Create station-year field
  stationyears <- data %>%
    mutate(STATION_YEAR = paste(site_no, Year, sep = "_"))

  ## How many unique station-years currently?
  length(unique(stationyears$STATION_YEAR)) # 305 unique site years
  
  ## Which station-years have at least 3 measurements for each variable?
  x <- stationyears %>%
     group_by(STATION_YEAR) %>%
      summarise(
        N_CHLA = sum(!is.na(CHLa)),
        N_TN = sum(!is.na(TN)),
        N_TP = sum(!is.na(TP)),
        N_TSS = sum(!is.na(TSS)),
      )
  
  index_chla <- x$STATION_YEAR[x$N_CHLA >= 3]
  index_nitrogen <- x$STATION_YEAR[x$N_TN >= 3]
  index_phosphorus <- x$STATION_YEAR[x$N_TP >= 3]
  index_TSS<- x$STATION_YEAR[x$N_TSS >= 3]

  
# This list will be used to loop through and subset each variable in its own df where station-year Nobs >= 3. The resulting list of dfs will eventually be used for mblm trend estimates.
variables <- list(
  CHLa = "index_chla",
  TN = "index_nitrogen",
  TP = "index_phosphorus",
  TSS = "index_TSS"
)

# Loop through variables and subset data
analysis.dfs <- lapply(names(variables), function(var) {
  
  index_var <- get(variables[[var]])  
  stationyears %>%
    filter(STATION_YEAR %in% index_var) %>%
    select(site_no, date, all_of(var), Year, Month, STATION_YEAR)
  
})

# Use variables list to name the resulting data frames
names(analysis.dfs) <- names(variables)

```
# Single data set for all wrangled variables

Each variable of interest is in its own data frame within the `analysis.dfs` list. Each df has the same schema, with only the variable column differing. This chunk simply joins all of those data frames into a single data frame. 

```{r}
#  Join on all except the variable column. This avoids duplicate cols
join_columns <- c("site_no", "date", "Year", "Month", "STATION_YEAR")

# Use purrr::reduce to join all data frames based on the common columns
my_data <- reduce(analysis.dfs, full_join, by = join_columns)

write_csv(my_data, "data/river_data/processed_riverData.csv")

```

# Derive Station Metadata & Summary Statistics

```{r}
## For stations with no data for a given variable, minYear and maxYear are missing and return as `Inf` or `-Inf`. The `ifelse` function is used to replace these values with NA.
metadata <- data %>%
  group_by(site_no) %>%
  summarise(
    across(
      c(CHLa, TN, TP, TSS),      # 1. Specify variables (columns) to summarize
      list(                       # 2. Define the metrics to calculate for each variable
        minYear = ~ ifelse(is.infinite(min(Year[!is.na(.)])), NA, min(Year[!is.na(.)])),
        maxYear = ~ ifelse(is.infinite(max(Year[!is.na(.)])), NA, max(Year[!is.na(.)])),
        mean = ~ mean(., na.rm = TRUE),
        median = ~ median(., na.rm = TRUE),
        min = ~ ifelse(is.infinite(min(., na.rm = TRUE)), NA, min(., na.rm = TRUE)),
        max = ~ ifelse(is.infinite(max(., na.rm = TRUE)), NA, max(., na.rm = TRUE)),
        Nobs = ~ sum(!is.na(.)),
        nYears = ~ n_distinct(Year[!is.na(.)])
      ),
      .names = "{.col}_{.fn}"     # 3. Specify the naming convention for the output columns. {.fn} references the names of the list elements.
    )
  )


write_csv(metadata, "data/river_data/river_metadata_summaryStats.csv")

```

# Subset Data for Trend Analysis

```{r}
# Function to subset data based on a minimum years of observations threshold. All station-variables in `data` have already been filtered to include only those with min 3 measurements per year (May-Oct). 
# In this case, 5 years of observations is the threshold.
subset_variable_data <- function(metadata_column, variable, threshold = 5) {
  stations <- metadata %>%
    filter(!!sym(metadata_column) >= threshold) %>%
    pull(site_no)
  
  return(data %>%
           filter(site_no %in% stations) %>%
           filter(!is.na(!!sym(variable)))
  )
    
}

# Create the subset_data list using the function
subset_data <- list(
  CHLa = subset_variable_data("CHLa_nYears", "CHLa"),           
  TN = subset_variable_data("TN_nYears", "TN"), 
  TP = subset_variable_data("TP_nYears", "TP"), 
  TSS = subset_variable_data("TSS_nYears", "TSS")
)

```

# Derive May-Oct Averages by Station

```{r}
# Function to calculate the May-Oct average for each station-variable combination
stationAvgs <- lapply(names(subset_data), function(var_name) {
  subset_data[[var_name]] %>%
    group_by(site_no, Year) %>%
    summarise(mean_value = mean(.data[[var_name]], na.rm = TRUE), 
              .groups = "drop")
})

names(stationAvgs) <- names(subset_data)

# Combine each variable's data into a single tibble with an added 'variable' column
stationAvgs_combined <- bind_rows(
  lapply(names(stationAvgs), function(var_name) {
    stationAvgs[[var_name]] %>%
      mutate(variable = var_name)  # Add a 'variable' column for identification
  })
)

```

# MBLM Trend Analysis

Run a mblm trend analysis for each station across all available years, for each variable in the `subset_data` list. 

```{r, warning = FALSE}
  
  modelSummaries <- list()  # Initialize list to store model results
  stations <- unique(stationAvgs_combined$site_no)  # Get unique station IDs
  vars <- c("CHLa", "TN", "TP", "TSS")
  
  for (site in stations) {
    site_trends <- list() # Initialize list to store trends for the current station
  
    df <- stationAvgs_combined %>%
      filter(site_no == site)  # Subset data for the current station
    
    for (var in vars) {
      df.var <- df %>%
        filter(variable == var)  # Subset data for the current variable
      if (nrow(df.var > 0)) { # Check if there are observations for the current variable
        model <- mblm::mblm(mean_value ~ Year, data = df.var)
        mod.sum <- mblm::summary.mblm(model)
        
        # Store results
        modelSummary <- list(
          station = site,
          variable = var,
          slope = mod.sum$coefficients[2, 1],
          MAD = mod.sum$coefficients["Year", "MAD"],
          pvalue = mod.sum$coefficients["Year", 4],
          intercept = mod.sum$coefficients[1, 1]
        )
        
        site_trends[[paste(var)]] <- modelSummary  # Append to list
      }
    }
    modelSummaries[[paste(site)]] <- site_trends  # Store trends for the current station
  }


```

# Extract Regression Statistics

```{r}
regression_statistics <- list()

for (station in names(modelSummaries)) {
      
  modelSums <- modelSummaries[[paste(station)]]
  
      reg.df <- data.frame(
              site_no = station,
              variable = vars[vars %in% names(modelSums)],
               model_slope = rep(NA, length(modelSums)),
               model_MAD = rep(NA, length(modelSums)),
               model_pval = rep(NA, length(modelSums)),
               model_intercept = rep(NA, length(modelSums))
      )
      
  for (i in seq_along(modelSums)) {
      
        p <- modelSums[[i]]
      
        reg.df$model_slope[i] <-  p$slope
        reg.df$model_MAD[i] <-  p$MAD
        reg.df$model_pval[i] <-  p$pvalue
        reg.df$model_intercept[i] <-  p$intercept
      }
      
      regression_statistics[[station]] <- reg.df
}

# ------- Create data frames from lists -------
regressions_df <- bind_rows(regression_statistics) 

regressions_df_wide <- regressions_df %>%
  pivot_wider(
    names_from = variable, 
    values_from = c(model_slope, model_MAD, model_pval, model_intercept),
    names_sep = "_"
  ) %>%
  # Reorder columns so all variables are grouped together column-wise
  select(
    site_no,
    contains("CHLa"),
    contains("TN"),
    contains("TP"),
    contains("TSS")
  )

```

# Adjusted Y-intercept

Because the model intercept is based on x=0, and the x-axes start around 1999, I create an adjusted model intercept based on the following formula:

`adjusted_intercept = model_intercept + model_slope * min(Year)`

This requires first determining the minimum year for each variable at each station

```{r}
minYear <- list()

for (var in vars) {
  
  df.var <- stationAvgs[[var]]
  minYear.var <- list()
  
  for (station in unique(df.var$site_no)) {
    
    df <- df.var %>%
      filter(site_no == station)
    
    minYear.df <- data.frame(variable = var,
                             site_no = as.character(station),
                             minYear = min(df$Year))
    
    minYear.var[[paste(var, station)]] <- minYear.df
  }
  
  minYear[[var]] <- bind_rows(minYear.var)
}

minYear.df <- bind_rows(minYear)

```

Then use that data frame to calculate the adjusted intercept

```{r}
regressions_adjusted <- regressions_df %>%
  left_join(minYear.df, by = c("site_no", "variable")) %>%
  mutate(adjusted_intercept = model_intercept + model_slope * minYear)

write_csv(regressions_adjusted, "data/river_data/stationTrends.csv")

```

# Generate Plots

```{r, fig.width=3, fig.height=4}
library(ggplot2)
# use regressions_adjusted along with the monthly averages list
# monthly avg list structure: list[[station]]$variable  variable = df with Year, STATION_ID, MAY_OCT_AVG
# recall that, due to thow the output of the `mblm` function is structured, it cannot be passed to geom_smooth as a smoothing function. This results in having to 'manually' draw the trend lines.
# List to hold all plots 
all_plots_list <- list()

for (station in unique(regressions_adjusted$site_no)) {
  # Initiate list to store plots for this station
  station_plots <- list()
  
  for (current_var in names(modelSummaries[[station]]))  {
    df_avgs <- stationAvgs[[current_var]] %>%
      filter(site_no == station)
    df_reg <- regressions_adjusted %>%
      filter(variable == current_var, site_no == station)
    
    var_plot <- ggplot(df_avgs, aes(x = Year, y = mean_value)) +
      geom_point() +
        # Use annotate() instead of geom_segment() for the trend line. ggplot expects each aesthetic  to be mapped to a column in the data, and geom_segment() - which I originally used - was receiving values (x, xend, y, and yend) that had only a single length (min and max). Also, aes() is designed to map aesthetics to a SINGLE dataframe, whereas this plot requires reference to two dfs to produce the trend lines.Using `annotate` eliminates the warning because it explicitly adds a line based on single values (rather than having ggplot2 try to match them to rows in df_avgs)
      annotate("segment", x = min(df_avgs$Year), 
               xend = max(df_avgs$Year), 
               y = df_reg$adjusted_intercept, 
               yend = 
                 df_reg$adjusted_intercept + df_reg$model_slope * (max(df_avgs$Year) - min(df_avgs$Year)),
               # calculate the y-value at the endpoint by adding the total change in y (slope * difference_in_years) to the intercept at the starting point
               color = "red", linewidth = .42, alpha = .6) +
      labs(title = current_var,
           subtitle = paste("Slope: ", round(df_reg$model_slope, 5),
                            "\np-value: ", round(df_reg$model_pval, 5)),
           x = NULL, y = NULL) +
      theme_minimal() +
      theme(plot.title = element_text(face = "bold"))
    
    station_plots[[current_var]] <- var_plot
  }

  all_plots_list[[station]] <- station_plots
}



save(all_plots_list, file = "riverStations_all_plots_list.RData")

```

```{r}
library(cowplot)
# Display each station's combined plots
for (station in names(all_plots_list)) {
  # Print the station header as an RMarkdown header (### is level 3)
  cat("### Station: ", station, "\n\n")
  
  # Use cowplot::plot_grid to combine plots and avoid appearance of "$variable" on axis
  combined_plot <- plot_grid(plotlist = all_plots_list[[station]], ncol = 2)
  print(combined_plot)
  
  # Add some space between stations
  cat("\n\n")
}

```