---
title: "Reservoir Water Clarity: Station-Level Monthly Trend Analysis"
author: "Andrew Cameron"
date: "2024-09-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
source("C:/Users/andre/my_functions.R")

```

```{r}
# Load the data
data <- openxlsx::read.xlsx("data/VADEQ_reservoir_data_original.xlsx", sheet = 2)

```


# Data Wrangling

The DEQ data need to be manipulated so that results from each station visit appear on a single row (StationID, date, Secchi, CHLa, etc.).

Fields in resulting data frame should be:

```
STATION_ID
DATE_TIME
SECCHI_DEPTH
CHLOROPHYLL-A UG/L SPECTROPHOTOMETRIC ACID. METH.
NITRITE PLUS NITRATE, TOTAL 1 DET. (MG/L AS NITROGEN)
NITROGEN, AMMONIA, TOTAL (MG/L AS N)
NITROGEN, TOTAL (MG/L AS N)
PHOSPHORUS, TOTAL (MG/L AS P)
PHOSPHORUS,IN TOTAL ORTHOPHOSPHATE (MG/L AS P)
SUSPENDED SEDIMENT, TOTAL, MG/L,(Method B)
TURBIDITY,HACH TURBIDIMETER (FORMAZIN TURB UNIT)
TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU
```

From Paul:

"The DEQ uses an odd hybrid format in which data are stored both in column and row format.  That is, some variables are stored in columns (Temperature, Secchi), whereas other variables are stored as rows (Chlorophyll, nutrients, etc.).  As a result, the data for a single station-month visit end up in multiple rows in which Temperature and Secchi depth are repeated on every row, and the other variables appear singly in each row.

One additional wrinkle, DEQ occasionally collects duplicate samples so there will be two rows for each variable for a single station-month visit.  In these cases, you can either average the two values or just take the first one.

One other potential point of confusion, the DEQ has two variables for chlorophyll.  We are using only one of these."

```{r}
# Variables of interest
variables <- c(
  "CHLOROPHYLL-A UG/L SPECTROPHOTOMETRIC ACID. METH.",
  "NITROGEN, TOTAL (MG/L AS N)",
  "PHOSPHORUS, TOTAL (MG/L AS P)",
  "PHOSPHORUS,IN TOTAL ORTHOPHOSPHATE (MG/L AS P)",
  "TURBIDITY,HACH TURBIDIMETER (FORMAZIN TURB UNIT)",
  "TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU"
)

# ---------- Pare down data to variables and months of interest ------------
df <- data %>%
  mutate(DATE_TIME = convertExcelDateTime(data, "DATE_TIME")) %>%   # Convert Excel date-time using custom function stored in `my_functions.R`
  filter(NAME %in% variables) %>%
  filter(month(DATE_TIME) >= 5 & month(DATE_TIME) <= 10) %>% # filter for months May through October
  mutate(DATE = date(DATE_TIME),
         StationDate = paste(STATION_ID, DATE, sep = "_")) %>%
  relocate(DATE, .after = "DATE_TIME")


# --------- Remove duplicate rows ------------
## I initially removed duplicates by grouping using DATE_TIME. I later realized that many station dates had duplicate entries for a given variable (NAME), though the duplicates were not necessarily at the same **time**. I then created a DATE field for grouping. This resulted in the the number of identified duplicate rows increasing from ~4,500 to ~20,000
index <- which(duplicated(df %>% select(STATION_ID, DATE, NAME)))
length(index) 

# --------- Pivot data to wide format ------------
wide_df <- df %>%
  select(STATION_ID, DATE, NAME, UNCENSORED_VALUE) %>%
  group_by(STATION_ID, DATE, NAME) %>%
  summarize(UNCENSORED_VALUE = mean(UNCENSORED_VALUE, na.rm = TRUE)) %>%  # Take the mean if there are duplicates
  pivot_wider(
    names_from = NAME, 
    values_from = UNCENSORED_VALUE
  ) %>%
  mutate(StationDate = paste(STATION_ID, DATE, sep = "_"),
         Year = year(DATE),
          Month = month(DATE))

nrow(unique(df[, c("DATE", "STATION_ID")])) == nrow(wide_df)  # Pivoting has successfully produced a dataframe with the same number of rows as there are unique station-month visits during May-Oct

# ---------- Reintegrate Secchi depth ------------- 
## Secchi depth was not used in pivoting because some combinations of station-date had multiple entries for Secchi depth.Using it in pivot results in multiple rows for each unique combination of station and date
secchi_df <- data %>%
  mutate(DATE_TIME = convertExcelDateTime(data, "DATE_TIME")) %>%
  filter(month(DATE_TIME) >= 5 & month(DATE_TIME) <= 10) %>% # filter for months May through October
  mutate(DATE = date(DATE_TIME),
         StationDate = paste(STATION_ID, DATE, sep = "_")) %>%
  select(STATION_ID, StationDate, SECCHI_DEPTH) %>%
  group_by(StationDate) %>%
  summarize(SECCHI_DEPTH = first(na.omit(SECCHI_DEPTH)))

  ## Join back to wide_df
  joined_df <- wide_df %>%
    left_join(
      secchi_df,
      by = "StationDate") %>%
    rename(
      CHLa = `CHLOROPHYLL-A UG/L SPECTROPHOTOMETRIC ACID. METH.`,
      NITROGEN_TOTAL = `NITROGEN, TOTAL (MG/L AS N)`,
      PHOSPHORUS_TOTAL = `PHOSPHORUS, TOTAL (MG/L AS P)`,
      PHOSPHORUS_ORTHO = `PHOSPHORUS,IN TOTAL ORTHOPHOSPHATE (MG/L AS P)`,
      TURBIDITY_HACH = `TURBIDITY,HACH TURBIDIMETER (FORMAZIN TURB UNIT)`,
      TURBIDITY_NTU = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`
    )
  
  joined_df %>%
    select(Year, STATION_ID, StationDate, NITROGEN_TOTAL) -> tn

```

### Incorporate Lake Anna Supplemental Data

"The task is to merge the two datasets, re-run the Station MetaData analysis for the merged dataset so we can see which stations gained years, and then re-run the trend analysis to see how the trend estimates were affected by having the additional data.  Note that for the trend analysis we are still using May-Oct averages, but with the combined DEQ-LACA dataset, we might have 2 observations in the same month, which is OK (you do not need to average by month first, just use the average of all observations).  Let's stick with the min of 3 observations in a year, but for the merged dataset, the min may not be 3 different months (i.e., we would include a year that might have 2 observations in July and 1 in August)."


```{r}
LAsupp.df <- openxlsx::read.xlsx("data/Lake Anna Suppl.xlsx", sheet = 2)

# Rename columns to match the DEQ data
LAsupp.df <- LAsupp.df %>%
  rename(
    DATE = "Sample.Date",
    STATION_ID = "DEQ.Site.#",
    CHLa = "CHLA.(ug/L)",
    SECCHI_DEPTH = "SECCHI.DEPTH.(m)",
    NITROGEN_TOTAL = "TOTNIT.(mg/L)",
    PHOSPHORUS_TOTAL = "TOTPHOS.(mg/L)"
  ) 

# Render date as date object with custom function
LAsupp.df$DATE <- convertExcelDateTime(LAsupp.df, "DATE")

# Create StationDate, StationYear, Year, and Month fields
LAsupp.df <- LAsupp.df %>%
  mutate(
    StationDate = paste(STATION_ID, DATE, sep = "_"),
    Year = year(DATE),
    Month = month(DATE)
  ) %>%
  filter(month(DATE) >= 5 & month(DATE) <= 10) %>%  # filter for months May through October
  select(-Rainfall, -LOCATION.NAME)

# Remove duplicate station-dates by taking the mean of the relevant variables
LAsupp.df <- LAsupp.df %>%
  group_by(StationDate) %>%
  summarize(across(c(STATION_ID, DATE, Year, Month), first),
            across(c(CHLa, SECCHI_DEPTH, NITROGEN_TOTAL, PHOSPHORUS_TOTAL), ~ mean(.x, na.rm = TRUE))) 


# Identify missing columns in LAsupp.df and add them with NA values
missing_columns <- setdiff(names(joined_df), names(LAsupp.df))

LAsupp.df[missing_columns] <- NA  # Add missing columns and fill with NA

combined_data <- bind_rows(joined_df, LAsupp.df)

# Combining produced a small number of duplicate station-date entries. Remove these.
combined_data <- combined_data %>%
  group_by(StationDate) %>%
  summarize(across(c(STATION_ID, DATE, Year, Month), first),
            across(c(CHLa, SECCHI_DEPTH, NITROGEN_TOTAL, PHOSPHORUS_TOTAL, PHOSPHORUS_ORTHO, TURBIDITY_HACH, TURBIDITY_NTU), ~ mean(.x, na.rm = TRUE)))

combined_data %>%
  select(Year, STATION_ID, StationDate, NITROGEN_TOTAL) %>%
  filter(!is.na(NITROGEN_TOTAL))-> tn



```


```{r}
# --------- Remove site years that do not have a minimum of 3 measurements in May-Oct ------------
  ## Create station-year field
  stationyears <- combined_data %>%
    mutate(STATION_YEAR = paste(STATION_ID, Year, sep = "_"))

  ## How many unique station-years currently?
  length(unique(stationyears$STATION_YEAR)) # 3750 unique site years
  
  ## Which station-years have at least 3 measurements for each variable?
  x <- stationyears %>%
     group_by(STATION_YEAR) %>%
      summarise(
        N_CHLA = sum(!is.na(CHLa)),
        N_SECCHI = sum(!is.na(SECCHI_DEPTH)),
        N_NITROGEN = sum(!is.na(NITROGEN_TOTAL)),
        N_PHOSPHORUS = sum(!is.na(PHOSPHORUS_TOTAL)),
        N_PHOSPHORUS_ORTHO = sum(!is.na(PHOSPHORUS_ORTHO)),
        N_TURBIDITY_HACH = sum(!is.na(TURBIDITY_HACH)),
        N_TURBIDITY_NTU = sum(!is.na(TURBIDITY_NTU))
      )
  
  index_chla <- x$STATION_YEAR[x$N_CHLA >= 3]
  index_secchi <- x$STATION_YEAR[x$N_SECCHI >= 3]
  index_nitrogen <- x$STATION_YEAR[x$N_NITROGEN >= 3]
  index_phosphorus <- x$STATION_YEAR[x$N_PHOSPHORUS >= 3]
  index_phosphorus_ortho <- x$STATION_YEAR[x$N_PHOSPHORUS_ORTHO >= 3]
  index_turbidity_hach <- x$STATION_YEAR[x$N_TURBIDITY_HACH >= 3]
  index_turbidity_ntu <- x$STATION_YEAR[x$N_TURBIDITY_NTU >= 3]
  
# This list will be used to loop through and subset each variable in its own df where station-year Nobs >= 3. The resulting list of dfs will eventually be used for mblm trend estimates.
variables <- list(
  CHLa = "index_chla",
  SECCHI_DEPTH = "index_secchi",
  NITROGEN_TOTAL = "index_nitrogen",
  PHOSPHORUS_TOTAL = "index_phosphorus",
  PHOSPHORUS_ORTHO = "index_phosphorus_ortho",
  TURBIDITY_HACH = "index_turbidity_hach",
  TURBIDITY_NTU = "index_turbidity_ntu"
)

# Loop through variables and subset data
analysis.dfs <- lapply(names(variables), function(var) {
  
  index_var <- get(variables[[var]])  
  stationyears %>%
    filter(STATION_YEAR %in% index_var) %>%
    select(STATION_ID, DATE, all_of(var), Year, Month, StationDate, STATION_YEAR)
  
})

# Use variables list to name the resulting data frames
names(analysis.dfs) <- names(variables)

  ## Clean up environment
  rm(data, df, wide_df, index, variables, stationyears, x, secchi_df, joined_df)

```



### DEQ Solids Data

After initital wrangling, summarizing, and overall assessment of available data, Paul found another DEQ dataset that contains alternative metrics for estimating suspended solids (TF & TSF). This is valuable because of the dearth of available TSS data in the original data set.

Solids data set has more or less the same sort of structure as the original data set (i.e., variables in both row and column format). After wrangling it must be incorporated into the main data set.


```{r}
solids_data <- openxlsx::read.xlsx("data/DEQ Solids Data.xlsx", sheet = 2)

# Variables of interest
variables <- c(
  "TF RESIDUE, TOTAL FIXED (MG/L) TOTAL FIXED SOLIDS", 
  "TSF RESIDUE, FIXED NONFILTRABLE (MG/L) TOTAL SUSPENDED FIXED SOLIDS")

# ---------- Pare down data to variables and months of interest ------------
df <- solids_data %>%
  mutate(DATE_TIME = convertExcelDateTime(solids_data, "DATE_TIME")) %>%
  filter(NAME %in% variables) %>%
  filter(month(DATE_TIME) >= 5 & month(DATE_TIME) <= 10) %>% # filter for months May through October
  mutate(DATE = date(DATE_TIME)) %>%
  relocate(DATE, .after = "DATE_TIME")

# --------- Pivot data to wide format ------------
wide_df <- df %>%
  select(STATION_ID, DATE, NAME, UNCENSORED_VALUE) %>%
  group_by(STATION_ID, DATE, NAME) %>%
  summarize(UNCENSORED_VALUE = mean(UNCENSORED_VALUE, na.rm = TRUE)) %>%  # Take the mean if there are duplicates
  pivot_wider(
    names_from = NAME, 
    values_from = UNCENSORED_VALUE
  ) %>%
  mutate(Year = year(DATE),
         Month = month(DATE),
         StationDate = paste(STATION_ID, DATE, sep = "_"))

nrow(unique(df[, c("DATE", "STATION_ID")])) == nrow(wide_df)  # Pivoting has successfully produced a dataframe with the same number of rows as there are unique station-month visits during May-Oct

# --------- Remove site years that do not have a minimum of 3 measurements in May-Oct ------------
  ## Create station-year field
  stationyears <- wide_df %>%
    mutate(STATION_YEAR = paste(STATION_ID, Year, sep = "_"))

  ## How many unique site years currently?
  length(unique(stationyears$STATION_YEAR)) # 3662 unique site years
  
  ## How many unique site years have at least 3 measurements for each variable?
  x <- stationyears %>%
     group_by(STATION_YEAR) %>%
      summarise(
        N_TSF = sum(!is.na(`TSF RESIDUE, FIXED NONFILTRABLE (MG/L) TOTAL SUSPENDED FIXED SOLIDS`)),
        N_TF = sum(!is.na(`TF RESIDUE, TOTAL FIXED (MG/L) TOTAL FIXED SOLIDS`))
      )
  
  index_tsf <- x$STATION_YEAR[x$N_TSF >= 3]
  index_tf <- x$STATION_YEAR[x$N_TF >= 3]
  
# ---------  Wrangled data frames ---------  
  ## Keep only site years with >= 3 measurements
  TSF_wrangled <- stationyears %>%
    filter(STATION_YEAR %in% index_tsf) %>%
    select(-`TF RESIDUE, TOTAL FIXED (MG/L) TOTAL FIXED SOLIDS`) %>%
    rename(TSF = "TSF RESIDUE, FIXED NONFILTRABLE (MG/L) TOTAL SUSPENDED FIXED SOLIDS")
  TF_wrangled <- stationyears %>%
    filter(STATION_YEAR %in% index_tf) %>%
    select(-`TSF RESIDUE, FIXED NONFILTRABLE (MG/L) TOTAL SUSPENDED FIXED SOLIDS`) %>%
    rename( TF = "TF RESIDUE, TOTAL FIXED (MG/L) TOTAL FIXED SOLIDS")

# Add wrangled, variable-specific data to the analysis.dfs list
analysis.dfs$TSF <- TSF_wrangled
analysis.dfs$TF <- TF_wrangled

  
  ## Clean up environment
  rm(solids_data, df, wide_df, variables, stationyears, x, index_tsf, index_tf, TSF_wrangled, TF_wrangled)

```

### TSS, Nitrate, Ammonia, and DIN

Subsequent development (9/17) - from Paul:

"There has been some confusion regarding what is the best variable to use to represent suspended sediments.  You'll recall that the variable originally chosen (Suspended Sediment) had very few observations, and so it was recommended I use TF or TSF.  But now it seems that TSS is the variable we want. I have added a page to the file DEQ Solids Data for TSS values.

The TN trend results look very interesting as it appears that TN is decreasing across the large majority of sites, and at a fast rate (~1.8 %/y = ~36% over 20-y of monitoring).  I would like to check whether this decline is due to decreases in Dissolved Inorganic Nitrogen.  There are two component variables, which are Nitrate and Ammonia (DIN is the sum of these two values).  We can only calculate DIN where both Nitrate and Ammonia have been reported.  It appears that DEQ measures Ammonia more often than Nitrate, so the latter may be the limiting factor, although there may be cases where Nitrate was measured but not Ammonia (page added: Nitrogen).

I would like for you to generate trend estimates for TSS, Nitrate, Ammonia and DIN using the same rules as for the other variables (i.e., only observations during May-Oct, remove duplicate station-dates and years with fewer than 3 monthly observations) and run mblms for stations with >= 15 observations."

```{r `TSS`}
tss_data <- openxlsx::read.xlsx("data/DEQ Solids Data.xlsx", sheet = 3)

# There is only one value in the NAME field
unique(tss_data$NAME)

# ---------- Pare down data to months of interest ------------
df <- tss_data %>%
  mutate(DATE_TIME = convertExcelDateTime(tss_data, "DATE_TIME")) %>%
  filter(month(DATE_TIME) >= 5 & month(DATE_TIME) <= 10) %>% # filter for months May through October
  mutate(DATE = date(DATE_TIME)) %>%
  relocate(DATE, .after = "DATE_TIME")

# --------- Pivot data to wide format ------------
wide_df <- df %>%
  select(STATION_ID, DATE, NAME, UNCENSORED_VALUE) %>%
  group_by(STATION_ID, DATE, NAME) %>%
  summarize(UNCENSORED_VALUE = mean(UNCENSORED_VALUE, na.rm = TRUE)) %>%  # Take the mean if there are duplicates
  pivot_wider(
    names_from = NAME, 
    values_from = UNCENSORED_VALUE
  ) %>%
  mutate(Year = year(DATE),
         Month = month(DATE),
         StationDate = paste(STATION_ID, DATE, sep = "_"))

nrow(unique(df[, c("DATE", "STATION_ID")])) == nrow(wide_df)  # Pivoting has successfully produced a dataframe with the same number of rows as there are unique station-month visits during May-Oct

# --------- Remove site years that do not have a minimum of 3 measurements in May-Oct ------------
  ## Create station-year field
  stationyears <- wide_df %>%
    mutate(STATION_YEAR = paste(STATION_ID, Year, sep = "_"))

  ## How many unique site years currently?
  length(unique(stationyears$STATION_YEAR)) # 1301 unique site years
  
  ## How many unique site years have at least 3 measurements?
  x <- stationyears %>%
    group_by(STATION_YEAR) %>%
    summarise(n = n())
  
  length(x$STATION_YEAR[x$n >= 3]) # 816 site years have at least 3 measurements
  index_stations <- x$STATION_YEAR[x$n >= 3]
  
  ## Keep only site years with >= 3 measurements
  y <- stationyears %>%
    filter(STATION_YEAR %in% index_stations) # 485 site years removed due to insufficient Nobs

  
# --------- Wrangled dataframe ------------
  tss_wrangled <- y %>%
    rename(TSS = "TSS RESIDUE, TOTAL NONFILTRABLE (MG/L) TOTAL SUSPENDED SOLIDS")
  
# Add wrangled, variable-specific data to the analysis.dfs list
analysis.dfs$TSS <- tss_wrangled
  
  ## Clean up environment
  rm(tss_data, tss_wrangled, df, wide_df, stationyears, x, y, index_stations)

```

```{r `Nitrate, Ammonia}
N_data <- openxlsx::read.xlsx("data/DEQ Solids Data.xlsx", sheet = 4)

# I am proceeding with the assumption that, despite 4 unique values for the NAME field, this data set only reflects two distinct measurement types.
N_data$NAME <- gsub("NITROGEN, AMMONIA, TOTAL DISSOLVED (MG/L AS N)", "NITROGEN, AMMONIA, TOTAL (MG/L AS N)", N_data$NAME, fixed=TRUE)
N_data$NAME <- gsub("NITRATE NITROGEN, DISSOLVED (MG/L AS N)", "NITRATE NITROGEN, TOTAL (MG/L AS N)", N_data$NAME, fixed = TRUE)


# ---------- Pare down data to months of interest ------------
df <- N_data %>%
  mutate(DATE_TIME = convertExcelDateTime(N_data, "DATE_TIME")) %>%
  filter(month(DATE_TIME) >= 5 & month(DATE_TIME) <= 10) %>% # filter for months May through October
  mutate(DATE = date(DATE_TIME)) %>%
  relocate(DATE, .after = "DATE_TIME")

# --------- Pivot data to wide format ------------
wide_df <- df %>%
  select(STATION_ID, DATE, NAME, UNCENSORED_VALUE) %>%
  group_by(STATION_ID, DATE, NAME) %>%
  summarize(UNCENSORED_VALUE = mean(UNCENSORED_VALUE, na.rm = TRUE)) %>%  # Take the mean if there are duplicates
  pivot_wider(
    names_from = NAME, 
    values_from = UNCENSORED_VALUE
  ) %>%
  mutate(Year = year(DATE),
         Month = month(DATE),
         StationDate = paste(STATION_ID, DATE, sep = "_"))

nrow(unique(df[, c("DATE", "STATION_ID")])) == nrow(wide_df)  # Pivoting has successfully produced a dataframe with the same number of rows as there are unique station-month visits during May-Oct

# --------- Remove site years that do not have a minimum of 3 measurements in May-Oct ------------
  ## Create station-year field
  stationyears <- wide_df %>%
    mutate(STATION_YEAR = paste(STATION_ID, Year, sep = "_")) %>%
    rename( AMMONIA_TOTAL = "NITROGEN, AMMONIA, TOTAL (MG/L AS N)") %>%
    rename( NITRATE_TOTAL = "NITRATE NITROGEN, TOTAL (MG/L AS N)")

  ## How many unique site years currently?
  length(unique(stationyears$STATION_YEAR)) # 3527 unique site years
  
  ## How many unique site years have at least 3 measurements for each variable?
  x <- stationyears %>%
     group_by(STATION_YEAR) %>%
      summarise(
        N_NITROGEN_AMMONIA = sum(!is.na(AMMONIA_TOTAL)),
        N_NITRATE_TOTAL = sum(!is.na(NITRATE_TOTAL))
      )
  
  index_ammonia <- x$STATION_YEAR[x$N_NITROGEN_AMMONIA >= 3]
  index_nitrate <- x$STATION_YEAR[x$N_NITRATE_TOTAL >= 3]
  
  
  ## Keep only site years with >= 3 measurements
  ammonia_wrangled <- stationyears %>%
    filter(STATION_YEAR %in% index_ammonia) %>%
    select(-NITRATE_TOTAL)

  nitrate_wrangled <- stationyears %>%
    filter(STATION_YEAR %in% index_nitrate) %>%
    select(-AMMONIA_TOTAL) 

  # Add wrangled, variable-specific data to the analysis.dfs list
  analysis.dfs$AMMONIA <- ammonia_wrangled
  analysis.dfs$NITRATE <- nitrate_wrangled

# -------- DIN -----------
  ## DIN is the sum of Nitrate and Ammonia. I will create a new variable in the data frame that is the sum of these two variables.
  DIN_wrangled <- stationyears %>%
    filter(STATION_YEAR %in% index_ammonia & STATION_YEAR %in% index_nitrate) %>%
    mutate(DIN = AMMONIA_TOTAL + NITRATE_TOTAL) %>%
    select(STATION_ID, DATE, DIN, Year, Month, StationDate, STATION_YEAR)
  
  analysis.dfs$DIN <- DIN_wrangled
  
  ## Clean up environment
  rm(N_data, df, wide_df, stationyears, x, index_ammonia, index_nitrate)

```

### Single data set for all wrangled variables

Each variable of interest is in its own data frame within the `analysis.dfs` list. Each df has the same schema, with only the variable column differing. This chunk simply joins all of those data frames into a single data frame. 

The incorporation of Lake Anna supplemental data added 276 total rows (station-dates) to the data set (455 station-dates in supplemental data after filtering out non May-Oct rows -- 179 of these were already in the DEQ data set).

```{r}
#  Join on all except the variable column. This avoids duplicate cols
join_columns <- c("STATION_ID", "DATE", "Year", "Month", "StationDate", "STATION_YEAR")

# Use purrr::reduce to join all data frames based on the common columns
my_data <- reduce(analysis.dfs, full_join, by = join_columns)

write_csv(my_data, "data/processed_dataset.csv")

```

Data must be further subset to include only lake stations that have a minimum of 15 years of observations. 
The result will be a separate subset of lake stations for each variable.
Using the joined df could *potentially* introduce station-years with <3 Nobs.
In that case, subsetting should be done on the individual, variable-specific data frames in the `analysis.dfs` list. 

The final processed data will have lots of NA values across variables, but this is expected because there is only one row per station-date (not every variable was measured at every station-date). The important thing is that no station-dates for any variables are included in the processed data that result in there being a year for a given variable in which there are fewer than 3 (i.e., 1 or 2) observations only during May-Oct. 

The code chunk below confirms that, in fact, after joining all data frames, no variables at the station-year level have fewer than 3 observations.

```{r}
  x <- my_data %>%
     group_by(STATION_YEAR) %>%
      summarise(
        N_CHLA = sum(!is.na(CHLa)),
        N_SECCHI = sum(!is.na(SECCHI_DEPTH)),
        N_NITROGEN = sum(!is.na(NITROGEN_TOTAL)),
        N_PHOSPHORUS = sum(!is.na(PHOSPHORUS_TOTAL)),
        N_PHOSPHORUS_ORTHO = sum(!is.na(PHOSPHORUS_ORTHO)),
        N_TURBIDITY_HACH = sum(!is.na(TURBIDITY_HACH)),
        N_TURBIDITY_NTU = sum(!is.na(TURBIDITY_NTU)),
        N_TSF = sum(!is.na(TSF)),
        N_TF = sum(!is.na(TF)),
        N_TSS = sum(!is.na(TSS)),
        N_AMMONIA = sum(!is.na(AMMONIA_TOTAL)),
        N_NITRATE = sum(!is.na(NITRATE_TOTAL)),
      )


any(x == 1)
any(x == 2)

```


# Extract Metadata for Stations

```{r}
# ----------- Extract metadata for stations ------------
## For stations with no data for a given variable, minYear and maxYear are missing and return as `Inf` or `-Inf`. The `ifelse` function is used to replace these values with NA.

metadata <- my_data %>%
  group_by(STATION_ID) %>%
  summarise(
    minYear_Secchi = ifelse(is.infinite(min(Year[!is.na(SECCHI_DEPTH)])), NA, min(Year[!is.na(SECCHI_DEPTH)])),
    maxYear_Secchi = ifelse(is.infinite(max(Year[!is.na(SECCHI_DEPTH)])), NA, max(Year[!is.na(SECCHI_DEPTH)])),
    nYears_Secchi = n_distinct(Year[!is.na(SECCHI_DEPTH)]),  # Count unique Years with Secchi data
    
    minYear_CHLa = ifelse(is.infinite(min(Year[!is.na(CHLa)])), NA, min(Year[!is.na(CHLa)])),
    maxYear_CHLa = ifelse(is.infinite(max(Year[!is.na(CHLa)])), NA, max(Year[!is.na(CHLa)])),
    nYears_CHLA = n_distinct(Year[!is.na(CHLa)]),
    
    minYear_totNitrogen = ifelse(is.infinite(min(Year[!is.na(NITROGEN_TOTAL)])), NA, min(Year[!is.na(NITROGEN_TOTAL)])),
    maxYear_totNitrogen = ifelse(is.infinite(max(Year[!is.na(NITROGEN_TOTAL)])), NA, max(Year[!is.na(NITROGEN_TOTAL)])),
    nYears_totNitrogen = n_distinct(Year[!is.na(NITROGEN_TOTAL)]),
    
    minYear_totPhosphorus = ifelse(is.infinite(min(Year[!is.na(PHOSPHORUS_TOTAL)])), NA, min(Year[!is.na(PHOSPHORUS_TOTAL)])),
    maxYear_totPhosphorus = ifelse(is.infinite(max(Year[!is.na(PHOSPHORUS_TOTAL)])), NA, max(Year[!is.na(PHOSPHORUS_TOTAL)])),
    nYears_totPhosphorus = n_distinct(Year[!is.na(PHOSPHORUS_TOTAL)]),
    
    minYear_TSF = ifelse(is.infinite(min(Year[!is.na(TSF)])), NA, min(Year[!is.na(TSF)])),
    maxYear_TSF = ifelse(is.infinite(max(Year[!is.na(TSF)])), NA, max(Year[!is.na(TSF)])),
    nYears_TSF = n_distinct(Year[!is.na(TSF)]),
    
    minYear_TF = ifelse(is.infinite(min(Year[!is.na(TF)])), NA, min(Year[!is.na(TF)])),
    maxYear_TF = ifelse(is.infinite(max(Year[!is.na(TF)])), NA, max(Year[!is.na(TF)])),
    nYears_TF = n_distinct(Year[!is.na(TF)]),
    
    minYear_TSS = ifelse(is.infinite(min(Year[!is.na(TSS)])), NA, min(Year[!is.na(TSS)])),
    maxYear_TSS = ifelse(is.infinite(max(Year[!is.na(TSS)])), NA, max(Year[!is.na(TSS)])),
    nYears_TSS = n_distinct(Year[!is.na(TSS)]),
    
    minYear_AMMONIA = ifelse(is.infinite(min(Year[!is.na(AMMONIA_TOTAL)])), NA, min(Year[!is.na(AMMONIA_TOTAL)])),
    maxYear_AMMONIA = ifelse(is.infinite(max(Year[!is.na(AMMONIA_TOTAL)])), NA, max(Year[!is.na(AMMONIA_TOTAL)])),
    nYears_AMMONIA = n_distinct(Year[!is.na(AMMONIA_TOTAL)]),
    
    minYear_NITRATE = ifelse(is.infinite(min(Year[!is.na(NITRATE_TOTAL)])), NA, min(Year[!is.na(NITRATE_TOTAL)])),
    maxYear_NITRATE = ifelse(is.infinite(max(Year[!is.na(NITRATE_TOTAL)])), NA, max(Year[!is.na(NITRATE_TOTAL)])),
    nYears_NITRATE = n_distinct(Year[!is.na(NITRATE_TOTAL)]),
    
    minYear_DIN = ifelse(is.infinite(min(Year[!is.na(DIN)])), NA, min(Year[!is.na(DIN)])),
    maxYear_DIN = ifelse(is.infinite(max(Year[!is.na(DIN)])), NA, max(Year[!is.na(DIN)])),
    nYears_DIN = n_distinct(Year[!is.na(DIN)]),
  )


write_csv(metadata, "data/updated_metadata.csv")


# ---- Compare Metadata before and after adding Lake Anna supplemental data ----
## How many stations now have 10+ yrs for each variable after adding Lake Anna supplemental data?
orig_meta <- read_csv("data/initital_metadata.csv")
vars_of_interest <- c("nYears_Secchi", "nYears_CHLA", "nYears_totNitrogen", "nYears_totPhosphorus")

# Summarise the original metadata
orig_summary <- orig_meta %>%
  summarise(
    n_stations = n(),
    across(vars_of_interest, ~ sum(. >= 10))
  )

# Summarise the updated metadata
updated_summary <- metadata %>%
  summarise(
    n_stations = n(),
    across(vars_of_interest, ~ sum(. >= 10))
  )

difference <- updated_summary - orig_summary

difference  # for each variable, there are 4 additional stations with 10+ years of data

## Which stations now have 10+ that previously did not?
# Step 1: Identify stations with 10+ years for each variable in the original metadata
orig_10plus <- orig_meta %>%
  filter(nYears_Secchi >= 10 | nYears_CHLA >= 10 | nYears_totNitrogen >= 10 | nYears_totPhosphorus >= 10) %>%
  select(STATION_ID)

# Step 2: Identify stations with 10+ years for each variable in the updated metadata
updated_10plus <- metadata %>%
  filter(nYears_Secchi >= 10 | nYears_CHLA >= 10 | nYears_totNitrogen >= 10 | nYears_totPhosphorus >= 10) %>%
  select(STATION_ID)

# Step 3: Find new stations with 10+ years by checking which STATION_IDs are in the updated metadata but not in the original metadata
new_10plus_stations <- updated_10plus %>%
  anti_join(orig_10plus, by = "STATION_ID")


print(new_10plus_stations) # 4 new stations in total

```

# Create Data Subsets by Variable

Subset data by variable where Nobs(years) >= some threhsold for that variable.

```{r}
subset_variable_data <- function(metadata_column, threshold = 10) {
  stations <- metadata %>%
    filter(!!sym(metadata_column) >= threshold) %>%
    pull(STATION_ID)
  
  return(my_data %>%
    filter(STATION_ID %in% stations))
}

# Create the subset_data list using the function
subset_data <- list(
  SECCHI_DEPTH = subset_variable_data("nYears_Secchi"),   
  CHLa = subset_variable_data("nYears_CHLA"),           
  NITROGEN_TOTAL = subset_variable_data("nYears_totNitrogen"), 
  PHOSPHORUS_TOTAL = subset_variable_data("nYears_totPhosphorus"), 
  TSF = subset_variable_data("nYears_TSF"), 
  TF = subset_variable_data("nYears_TF"),
  TSS = subset_variable_data("nYears_TSS"),
  AMMONIA_TOTAL = subset_variable_data("nYears_AMMONIA"),
  NITRATE_TOTAL = subset_variable_data("nYears_NITRATE"),
  DIN = subset_variable_data("nYears_DIN")
)

```

### Monthly summary statistics

Monthly summary stats by unique lake station (where n Years >= 15), per variable. 

```{r}
# ---- Custom Function to derive monthly summary statistics ----
monthlyStats <- function(variable) {
  
  df <- subset_data[[variable]]  
  
  monthlyStation_stats <- df %>%
    group_by(STATION_ID, Month) %>%
    summarize(Mean = mean(!!sym(variable), na.rm = TRUE),
              Min = min(!!sym(variable), na.rm = TRUE),
              Max = max(!!sym(variable), na.rm = TRUE),
              Nobs = n(),
              .groups = 'drop')
  
 return(monthlyStation_stats)
}

# ----- Apply function -------
variables <- c("SECCHI_DEPTH", "CHLa", "NITROGEN_TOTAL", "PHOSPHORUS_TOTAL", "TSF", "TF", "TSS", "AMMONIA_TOTAL", "NITRATE_TOTAL", "DIN")

Monthly_Summary_Statistics <- list()

for (variable in variables) {
  Monthly_Summary_Statistics[[variable]] <- monthlyStats(variable)
}



bind_rows(Monthly_Summary_Statistics, .id = "Variable") %>%
  write_csv("data/Monthly_Summary_Statistics.csv")

```


# Trends at the Station Level

#### Custom function to derive trends

```{r}
trends_func <- function(variable) {
  
  modelSummaries <- list()  
  
  df <- subset_data[[variable]]
  
  # Remove NA values (mblm cannot handle them)
  df <- df %>%
    filter(!is.na(!!sym(variable)))
  
  # Iterate over unique stations
  for (station_id in unique(df$STATION_ID)) {
    for (month_num in 5:10) {
      
      month_station <- df %>%
        filter(STATION_ID == station_id & Month == month_num)
      
      if (nrow(month_station) > 0) {  # Ensure data exists
        
        #  Fit models; catch errors if necessary
        tryCatch({
          model <- mblm::mblm(as.formula(paste(variable, "~ Year")), data = month_station)
          mod.sum <- mblm::summary.mblm(model)
          
          # Store results
          modelSummaries[[paste(station_id, month_num, sep = "_")]] <- 
            list(
              slope = mod.sum$coefficients[2, 1],
              MAD = mod.sum$coefficients["Year", "MAD"],
              pvalue = mod.sum$coefficients["Year", 4],
              intercept = mod.sum$coefficients[1, 1]
            )
        }, error = function(e) {
          # Handle the error, in this case generate message that points to problematic station and month
          message(paste("Error for", variable, "on", station:, station_id, "month:", month_num,":", e$message))
        })
      }
    }
  }
  
  return(modelSummaries)  # Return the results
}

```

### Run the function for each variable

```{r, warning=FALSE}
variables <- c("SECCHI_DEPTH", "CHLa", "NITROGEN_TOTAL", "PHOSPHORUS_TOTAL", "TSF", "TF", "TSS", "AMMONIA_TOTAL", "NITRATE_TOTAL", "DIN")

trends_results <- list()

# Loop through each variable and run the function. This generates a nested list structure:
# trends_results (outer list); contains -> list for each variable; contains -> lists for each station_month; which each contain -> 1 list with 4 elements (the model results)
for (variable in variables) {
  trends_results[[variable]] <- trends_func(variable)
}

```

### Extract and Store Regression Statistics 

```{r `add reg stats to monthly summary df`}
regression_statistics <- list()

for (i in seq_along(Monthly_Summary_Statistics)){
  
  variable = names(Monthly_Summary_Statistics[i])
  
  # create df from summary stats that includes station-month key and fields for model results
  reg.df <- Monthly_Summary_Statistics[[i]] %>%
    mutate(key = paste(STATION_ID, Month, sep="_"),
           model_slope = NA,
           model_MAD = NA,
           model_pval = NA,
           model_intercept = NA)
  
  # List with trend results for desired var
  modelSummaries <- trends_results[[variable]]
  
  for (j in seq_along(modelSummaries)) {
    key = names(modelSummaries[j])
    
    reg.df$model_slope[j] <-  modelSummaries[[key]]$slope
     reg.df$model_MAD[j] <-  modelSummaries[[key]]$MAD
      reg.df$model_pval[j] <-  modelSummaries[[key]]$pvalue
       reg.df$model_intercept[j] <-  modelSummaries[[key]]$intercept
    
  }
  
  regression_statistics[[variable]] <- reg.df
}


regressions_df <- bind_rows(regression_statistics, .id = "Variable")

write_csv(regressions_df, "data/monthly_RegressionStatistics.csv")

```

